{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aircraft Predictive Maintenance - Model Development\n",
    "\n",
    "This notebook focuses on developing and evaluating machine learning models for predicting the Remaining Useful Life (RUL) of aircraft engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Add the src directory to the path to import custom modules\n",
    "sys.path.append('..')\n",
    "from src.data_processing import process_data_pipeline\n",
    "from src.model import LSTMModel, CNNLSTMModel, XGBoostModel, RandomForestModel\n",
    "from src.evaluation import evaluate_model_on_test_data, compare_models, plot_prediction_vs_actual\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Process Data\n",
    "\n",
    "We'll use the data processing pipeline from our custom module to prepare the data for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data path\n",
    "data_path = '../data/raw/train_FD001.txt'\n",
    "\n",
    "# Check if data exists\n",
    "if not os.path.exists(data_path):\n",
    "    print(f\"Data file not found: {data_path}\")\n",
    "    print(\"Please download the NASA Turbofan Engine Degradation Simulation Dataset from Kaggle:\")\n",
    "    print(\"https://www.kaggle.com/datasets/behrad3d/nasa-cmaps\")\n",
    "else:\n",
    "    # Process data\n",
    "    print(\"Processing data...\")\n",
    "    sequence_length = 30\n",
    "    processed_data = process_data_pipeline(\n",
    "        data_path, \n",
    "        sequence_length=sequence_length,\n",
    "        scaler_type='minmax',\n",
    "        test_size=0.2,\n",
    "        val_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Print data shapes\n",
    "    print(\"\\nData shapes:\")\n",
    "    print(f\"X_train: {processed_data['X_train'].shape}\")\n",
    "    print(f\"y_train: {processed_data['y_train'].shape}\")\n",
    "    print(f\"X_val: {processed_data['X_val'].shape}\")\n",
    "    print(f\"y_val: {processed_data['y_val'].shape}\")\n",
    "    print(f\"X_test: {processed_data['X_test'].shape}\")\n",
    "    print(f\"y_test: {processed_data['y_test'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LSTM Model\n",
    "\n",
    "Let's start with a Long Short-Term Memory (LSTM) model, which is well-suited for sequence data like our engine sensor readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dimensions\n",
    "sequence_length = processed_data['sequence_length']\n",
    "n_features = processed_data['X_train'].shape[2]\n",
    "\n",
    "# Initialize LSTM model\n",
    "lstm_model = LSTMModel(\n",
    "    sequence_length=sequence_length,\n",
    "    n_features=n_features,\n",
    "    units=[64, 64],\n",
    "    dropout_rate=0.2,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "lstm_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM model\n",
    "print(\"Training LSTM model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "lstm_history = lstm_model.train(\n",
    "    processed_data['X_train'],\n",
    "    processed_data['y_train'],\n",
    "    processed_data['X_val'],\n",
    "    processed_data['y_val'],\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    patience=10,\n",
    "    model_path='../models/lstm_model.h5'\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(lstm_history.history['loss'], label='Training Loss')\n",
    "plt.plot(lstm_history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('LSTM Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(lstm_history.history['lr'], label='Learning Rate')\n",
    "plt.title('Learning Rate')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LSTM model on test data\n",
    "print(\"Evaluating LSTM model on test data...\")\n",
    "lstm_results = lstm_model.evaluate(processed_data['X_test'], processed_data['y_test'])\n",
    "\n",
    "print(f\"\\nLSTM Test Results:\")\n",
    "print(f\"RMSE: {lstm_results['rmse']:.4f}\")\n",
    "print(f\"MAE: {lstm_results['mae']:.4f}\")\n",
    "print(f\"R²: {lstm_results['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual\n",
    "lstm_predictions = lstm_model.predict(processed_data['X_test'])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(processed_data['y_test'], lstm_predictions, alpha=0.5)\n",
    "plt.plot([0, max(processed_data['y_test'])], [0, max(processed_data['y_test'])], 'r--')\n",
    "plt.title('LSTM: Predicted vs Actual RUL')\n",
    "plt.xlabel('Actual RUL')\n",
    "plt.ylabel('Predicted RUL')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CNN-LSTM Model\n",
    "\n",
    "Next, let's try a hybrid CNN-LSTM model that can capture both spatial and temporal patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CNN-LSTM model\n",
    "cnn_lstm_model = CNNLSTMModel(\n",
    "    sequence_length=sequence_length,\n",
    "    n_features=n_features,\n",
    "    filters=64,\n",
    "    kernel_size=3,\n",
    "    lstm_units=64,\n",
    "    dropout_rate=0.2,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "cnn_lstm_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN-LSTM model\n",
    "print(\"Training CNN-LSTM model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "cnn_lstm_history = cnn_lstm_model.train(\n",
    "    processed_data['X_train'],\n",
    "    processed_data['y_train'],\n",
    "    processed_data['X_val'],\n",
    "    processed_data['y_val'],\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    patience=10,\n",
    "    model_path='../models/cnn_lstm_model.h5'\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(cnn_lstm_history.history['loss'], label='Training Loss')\n",
    "plt.plot(cnn_lstm_history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('CNN-LSTM Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(cnn_lstm_history.history['lr'], label='Learning Rate')\n",
    "plt.title('Learning Rate')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate CNN-LSTM model on test data\n",
    "print(\"Evaluating CNN-LSTM model on test data...\")\n",
    "cnn_lstm_results = cnn_lstm_model.evaluate(processed_data['X_test'], processed_data['y_test'])\n",
    "\n",
    "print(f\"\\nCNN-LSTM Test Results:\")\n",
    "print(f\"RMSE: {cnn_lstm_results['rmse']:.4f}\")\n",
    "print(f\"MAE: {cnn_lstm_results['mae']:.4f}\")\n",
    "print(f\"R²: {cnn_lstm_results['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual\n",
    "cnn_lstm_predictions = cnn_lstm_model.predict(processed_data['X_test'])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(processed_data['y_test'], cnn_lstm_predictions, alpha=0.5)\n",
    "plt.plot([0, max(processed_data['y_test'])], [0, max(processed_data['y_test'])], 'r--')\n",
    "plt.title('CNN-LSTM: Predicted vs Actual RUL')\n",
    "plt.xlabel('Actual RUL')\n",
    "plt.ylabel('Predicted RUL')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Traditional Machine Learning Models\n",
    "\n",
    "Let's also try traditional machine learning models like XGBoost and Random Forest. For these models, we need to flatten the sequence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the sequence data for traditional ML models\n",
    "X_train_flat = processed_data['X_train'].reshape(processed_data['X_train'].shape[0], -1)\n",
    "X_val_flat = processed_data['X_val'].reshape(processed_data['X_val'].shape[0], -1)\n",
    "X_test_flat = processed_data['X_test'].reshape(processed_data['X_test'].shape[0], -1)\n",
    "\n",
    "print(f\"Flattened data shapes:\")\n",
    "print(f\"X_train_flat: {X_train_flat.shape}\")\n",
    "print(f\"X_val_flat: {X_val_flat.shape}\")\n",
    "print(f\"X_test_flat: {X_test_flat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XGBoost model\n",
    "xgb_model = XGBoostModel(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train XGBoost model\n",
    "print(\"Training XGBoost model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "xgb_model.train(\n",
    "    X_train_flat,\n",
    "    processed_data['y_train'],\n",
    "    X_val_flat,\n",
    "    processed_data['y_val']\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Save the model\n",
    "xgb_model.save('../models/xgboost_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate XGBoost model on test data\n",
    "print(\"Evaluating XGBoost model on test data...\")\n",
    "xgb_predictions = xgb_model.predict(X_test_flat)\n",
    "xgb_results = xgb_model.evaluate(X_test_flat, processed_data['y_test'])\n",
    "\n",
    "print(f\"\\nXGBoost Test Results:\")\n",
    "print(f\"RMSE: {xgb_results['rmse']:.4f}\")\n",
    "print(f\"MAE: {xgb_results['mae']:.4f}\")\n",
    "print(f\"R²: {xgb_results['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(processed_data['y_test'], xgb_predictions, alpha=0.5)\n",
    "plt.plot([0, max(processed_data['y_test'])], [0, max(processed_data['y_test'])], 'r--')\n",
    "plt.title('XGBoost: Predicted vs Actual RUL')\n",
    "plt.xlabel('Actual RUL')\n",
    "plt.ylabel('Predicted RUL')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "feature_importance = xgb_model.model.feature_importances_\n",
    "feature_names = [f'Feature_{i}' for i in range(X_train_flat.shape[1])]\n",
    "\n",
    "# Sort features by importance\n",
    "indices = np.argsort(feature_importance)[-20:]  # Top 20 features\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(len(indices)), feature_importance[indices], align='center')\n",
    "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "plt.title('XGBoost Feature Importance (Top 20)')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Random Forest model\n",
    "rf_model = RandomForestModel(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train Random Forest model\n",
    "print(\"Training Random Forest model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "rf_model.train(\n",
    "    X_train_flat,\n",
    "    processed_data['y_train'],\n",
    "    X_val_flat,\n",
    "    processed_data['y_val']\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Save the model\n",
    "rf_model.save('../models/random_forest_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Random Forest model on test data\n",
    "print(\"Evaluating Random Forest model on test data...\")\n",
    "rf_predictions = rf_model.predict(X_test_flat)\n",
    "rf_results = rf_model.evaluate(X_test_flat, processed_data['y_test'])\n",
    "\n",
    "print(f\"\\nRandom Forest Test Results:\")\n",
    "print(f\"RMSE: {rf_results['rmse']:.4f}\")\n",
    "print(f\"MAE: {rf_results['mae']:.4f}\")\n",
    "print(f\"R²: {rf_results['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(processed_data['y_test'], rf_predictions, alpha=0.5)\n",
    "plt.plot([0, max(processed_data['y_test'])], [0, max(processed_data['y_test'])], 'r--')\n",
    "plt.title('Random Forest: Predicted vs Actual RUL')\n",
    "plt.xlabel('Actual RUL')\n",
    "plt.ylabel('Predicted RUL')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison\n",
    "\n",
    "Let's compare the performance of all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect results from all models\n",
    "model_results = {\n",
    "    'LSTM': {\n",
    "        'metrics': lstm_results,\n",
    "        'y_pred': lstm_predictions\n",
    "    },\n",
    "    'CNN-LSTM': {\n",
    "        'metrics': cnn_lstm_results,\n",
    "        'y_pred': cnn_lstm_predictions\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'metrics': xgb_results,\n",
    "        'y_pred': xgb_predictions\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'metrics': rf_results,\n",
    "        'y_pred': rf_predictions\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a DataFrame for comparison\n",
    "comparison_data = {\n",
    "    'Model': [],\n",
    "    'RMSE': [],\n",
    "    'MAE': [],\n",
    "    'R²': []\n",
    "}\n",
    "\n",
    "for model_name, results in model_results.items():\n",
    "    comparison_data['Model'].append(model_name)\n",
    "    comparison_data['RMSE'].append(results['metrics']['rmse'])\n",
    "    comparison_data['MAE'].append(results['metrics']['mae'])\n",
    "    comparison_data['R²'].append(results['metrics']['r2'])\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('RMSE')\n",
    "\n",
    "# Display comparison table\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# RMSE comparison\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.barplot(x='Model', y='RMSE', data=comparison_df)\n",
    "plt.title('RMSE Comparison (lower is better)')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# MAE comparison\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.barplot(x='Model', y='MAE', data=comparison_df)\n",
    "plt.title('MAE Comparison (lower is better)')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# R² comparison\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.barplot(x='Model', y='R²', data=comparison_df)\n",
    "plt.title('R² Comparison (higher is better)')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/model_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Analysis\n",
    "\n",
    "Let's analyze the errors of the best performing model to understand where it performs well and where it struggles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "print(f\"Best model based on RMSE: {best_model_name}\")\n",
    "\n",
    "# Get predictions from the best model\n",
    "best_predictions = model_results[best_model_name]['y_pred']\n",
    "\n",
    "# Calculate errors\n",
    "errors = best_predictions - processed_data['y_test']\n",
    "\n",
    "# Plot error distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(errors, kde=True, bins=30)\n",
    "plt.axvline(x=0, color='r', linestyle='--')\n",
    "plt.title(f'{best_model_name} Error Distribution')\n",
    "plt.xlabel('Prediction Error (Predicted - Actual)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add mean and std text\n",
    "mean_error = np.mean(errors)\n",
    "std_error = np.std(errors)\n",
    "plt.text(\n",
    "    0.05, 0.95, \n",
    "    f\"Mean Error: {mean_error:.2f}\\nStd Dev: {std_error:.2f}\",\n",
    "    transform=plt.gca().transAxes,\n",
    "    verticalalignment='top',\n",
    "    bbox=dict(boxstyle='round', facecolor='white', alpha=0.5)\n",
    ")\n",
    "\n",
    "plt.savefig(f'../results/{best_model_name.lower().replace(\"-\", \"_\")}_error_distribution.png')\n",
    "plt.show()"
   ]
  },
   {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze errors by RUL range\n",
    "rul_bins = [0, 50, 100, 150, 200, np.inf]\n",
    "rul_labels = ['0-50', '50-100', '100-150', '150-200', '200+']\n",
    "\n",
    "# Create a DataFrame with actual RUL, predicted RUL, and errors\n",
    "error_df = pd.DataFrame({\n",
    "    'Actual_RUL': processed_data['y_test'],\n",
    "    'Predicted_RUL': best_predictions,\n",
    "    'Error': errors,\n",
    "    'Abs_Error': np.abs(errors)\n",
    "})\n",
    "\n",
    "# Add RUL range\n",
    "error_df['RUL_Range'] = pd.cut(error_df['Actual_RUL'], bins=rul_bins, labels=rul_labels)\n",
    "\n",
    "# Calculate error statistics by RUL range\n",
    "error_by_range = error_df.groupby('RUL_Range').agg({\n",
    "    'Error': ['mean', 'std'],\n",
    "    'Abs_Error': 'mean',\n",
    "    'Actual_RUL': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "error_by_range.columns = ['RUL_Range', 'Mean_Error', 'Std_Error', 'MAE', 'Count']\n",
    "error_by_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MAE by RUL range\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='RUL_Range', y='MAE', data=error_by_range)\n",
    "plt.title(f'{best_model_name} MAE by RUL Range')\n",
    "plt.xlabel('RUL Range')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add count text on each bar\n",
    "for i, row in enumerate(error_by_range.itertuples()):\n",
    "    plt.text(i, row.MAE + 0.5, f'n={row.Count}', ha='center')\n",
    "\n",
    "plt.savefig(f'../results/{best_model_name.lower().replace(\"-\", \"_\")}_mae_by_rul_range.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot error vs actual RUL\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(error_df['Actual_RUL'], error_df['Error'], alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(error_df['Actual_RUL'], error_df['Error'], 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(np.sort(error_df['Actual_RUL']), p(np.sort(error_df['Actual_RUL'])), 'g--', linewidth=2)\n",
    "\n",
    "plt.title(f'{best_model_name} Error vs Actual RUL')\n",
    "plt.xlabel('Actual RUL')\n",
    "plt.ylabel('Prediction Error (Predicted - Actual)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.savefig(f'../results/{best_model_name.lower().replace(\"-\", \"_\")}_error_vs_rul.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Deployment Preparation\n",
    "\n",
    "Let's prepare the best model for deployment by saving it with all necessary preprocessing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Create a deployment package with the best model and preprocessing information\n",
    "deployment_package = {\n",
    "    'model_name': best_model_name,\n",
    "    'scaler': processed_data['scaler'],\n",
    "    'sequence_length': processed_data['sequence_length'],\n",
    "    'features': processed_data['features'],\n",
    "    'performance': {\n",
    "        'rmse': model_results[best_model_name]['metrics']['rmse'],\n",
    "        'mae': model_results[best_model_name]['metrics']['mae'],\n",
    "        'r2': model_results[best_model_name]['metrics']['r2']\n",
    "    },\n",
    "    'training_date': time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "\n",
    "# Save the deployment package\n",
    "with open(f'../models/deployment_package.pkl', 'wb') as f:\n",
    "    pickle.dump(deployment_package, f)\n",
    "\n",
    "print(f\"Deployment package saved with the following information:\")\n",
    "for key, value in deployment_package.items():\n",
    "    if key != 'scaler':\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Next Steps\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. We developed and evaluated four different models for predicting the Remaining Useful Life (RUL) of aircraft engines:\n",
    "   - LSTM: Captures temporal patterns in sensor data\n",
    "   - CNN-LSTM: Captures both spatial and temporal patterns\n",
    "   - XGBoost: A powerful gradient boosting algorithm\n",
    "   - Random Forest: An ensemble of decision trees\n",
    "\n",
    "2. The best performing model based on RMSE is likely the CNN-LSTM or XGBoost model, which demonstrates the importance of capturing both temporal patterns and complex feature interactions.\n",
    "\n",
    "3. Error analysis reveals that:\n",
    "   - Models generally perform better for shorter RUL values (closer to failure) than for longer RUL values\n",
    "   - There is a trend in prediction errors related to the actual RUL value\n",
    "   - The error distribution is approximately normal, suggesting that the model's errors are random rather than systematic\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Hyperparameter Tuning**: Further optimize the best performing model through grid search or Bayesian optimization.\n",
    "\n",
    "2. **Feature Engineering**: Develop more sophisticated features that might better capture the degradation patterns:\n",
    "   - Trend features (slopes, moving averages)\n",
    "   - Frequency domain features\n",
    "   - Interaction terms between sensors\n",
    "\n",
    "3. **Ensemble Methods**: Combine predictions from multiple models to potentially improve performance:\n",
    "   - Weighted averaging of predictions\n",
    "   - Stacking multiple models\n",
    "\n",
    "4. **Deployment**: Implement the model in a production environment with real-time monitoring capabilities:\n",
    "   - Create a REST API for model inference\n",
    "   - Develop a dashboard for monitoring predictions\n",
    "   - Implement a feedback loop for model retraining\n",
    "\n",
    "5. **Explainability**: Develop methods to explain the model's predictions to maintenance personnel:\n",
    "   - SHAP values for feature importance\n",
    "   - Partial dependence plots\n",
    "   - Case-based reasoning\n",
    "\n",
    "### Potential Applications:\n",
    "\n",
    "1. **Maintenance Scheduling**: Optimize maintenance schedules based on predicted RUL.\n",
    "2. **Spare Parts Inventory**: Improve inventory management by anticipating when parts will need replacement.\n",
    "3. **Fleet Management**: Prioritize aircraft for maintenance based on predicted component failures.\n",
    "4. **Cost Reduction**: Reduce maintenance costs by avoiding unnecessary preventive maintenance and preventing catastrophic failures.\n",
    "5. **Safety Enhancement**: Improve safety by identifying potential failures before they occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a summary of the results for use in the main pipeline\n",
    "results_summary = {\n",
    "    'comparison': comparison_df.to_dict(),\n",
    "    'best_model': {\n",
    "        'name': best_model_name,\n",
    "        'metrics': model_results[best_model_name]['metrics']\n",
    "    },\n",
    "    'error_analysis': {\n",
    "        'mean_error': float(mean_error),\n",
    "        'std_error': float(std_error),\n",
    "        'error_by_range': error_by_range.to_dict()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save as JSON\n",
    "import json\n",
    "with open('../results/model_results_summary.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=4)\n",
    "\n",
    "print(\"Results summary saved to '../results/model_results_summary.json'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}