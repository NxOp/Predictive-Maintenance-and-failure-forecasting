

```python:c%3A%5CUsers%5Cnitya%5CDesktop%5Csrc%5Cdata_processing.py
"""
Data processing module for aircraft predictive maintenance.

This module handles loading, cleaning, and preprocessing of the NASA Turbofan 
Engine Degradation Simulation Dataset.
"""

import pandas as pd
import numpy as np
import os
from typing import Tuple, List, Dict
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_dataset(filepath: str) -> pd.DataFrame:
    """
    Load the raw dataset from the specified filepath.
    
    Args:
        filepath: Path to the dataset file
        
    Returns:
        DataFrame containing the raw data
    """
    logger.info(f"Loading dataset from {filepath}")
    
    try:
        # Determine file type and load accordingly
        if filepath.endswith('.csv'):
            df = pd.read_csv(filepath)
        elif filepath.endswith('.txt'):
            # NASA dataset specific format
            # Assuming space-separated values with no header
            column_names = ['engine_id', 'cycle', 'setting1', 'setting2', 'setting3'] + [f'sensor{i}' for i in range(1, 22)]
            df = pd.read_csv(filepath, sep=' ', header=None, names=column_names)
            # Remove NaN columns that might appear due to extra spaces
            df = df.dropna(axis=1, how='all')
        else:
            raise ValueError(f"Unsupported file format: {filepath}")
            
        logger.info(f"Successfully loaded dataset with shape {df.shape}")
        return df
    
    except Exception as e:
        logger.error(f"Error loading dataset: {str(e)}")
        raise

def clean_dataset(df: pd.DataFrame) -> pd.DataFrame:
    """
    Clean the dataset by handling missing values, outliers, etc.
    
    Args:
        df: Raw DataFrame
        
    Returns:
        Cleaned DataFrame
    """
    logger.info("Cleaning dataset...")
    
    # Make a copy to avoid modifying the original
    df_clean = df.copy()
    
    # DEV NOTE: After 3 days of trying different imputation methods, 
    # simple forward fill seems to work best for this time series data.
    # Tried KNN and MICE but they introduced too much noise.
    
    # Handle missing values
    missing_before = df_clean.isna().sum().sum()
    if missing_before > 0:
        # Forward fill for time series data
        df_clean = df_clean.fillna(method='ffill')
        # If there are still missing values (e.g., at the beginning), use backward fill
        df_clean = df_clean.fillna(method='bfill')
        # If there are STILL missing values, use column median
        df_clean = df_clean.fillna(df_clean.median())
        
        logger.info(f"Handled {missing_before} missing values")
    
    # Handle outliers using IQR method
    # DEV NOTE: Sigh... spent the whole weekend debating whether to remove outliers
    # or cap them. Capping seems less destructive for this particular dataset.
    for col in df_clean.select_dtypes(include=[np.number]).columns:
        if col not in ['engine_id', 'cycle']:  # Skip identifier columns
            Q1 = df_clean[col].quantile(0.25)
            Q3 = df_clean[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            # Cap outliers instead of removing them
            df_clean[col] = df_clean[col].clip(lower=lower_bound, upper=upper_bound)
    
    logger.info("Dataset cleaning completed")
    return df_clean

def calculate_RUL(df: pd.DataFrame) -> pd.DataFrame:
    """
    Calculate Remaining Useful Life (RUL) for each engine.
    
    Args:
        df: DataFrame with engine data
        
    Returns:
        DataFrame with added RUL column
    """
    logger.info("Calculating Remaining Useful Life (RUL)...")
    
    # Make a copy to avoid modifying the original
    df_with_rul = df.copy()
    
    # Group by engine_id and calculate max cycle for each engine
    max_cycles = df_with_rul.groupby('engine_id')['cycle'].max().reset_index()
    max_cycles.columns = ['engine_id', 'max_cycle']
    
    # Merge with original dataframe
    df_with_rul = df_with_rul.merge(max_cycles, on='engine_id', how='left')
    
    # Calculate RUL as the difference between max cycle and current cycle
    df_with_rul['RUL'] = df_with_rul['max_cycle'] - df_with_rul['cycle']
    
    # Drop the helper column
    df_with_rul = df_with_rul.drop('max_cycle', axis=1)
    
    logger.info("RUL calculation completed")
    return df_with_rul

def normalize_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:
    """
    Normalize numerical features to [0,1] range.
    
    Args:
        df: DataFrame to normalize
        
    Returns:
        Tuple of (normalized DataFrame, dictionary of min/max values)
    """
    logger.info("Normalizing data...")
    
    # Make a copy to avoid modifying the original
    df_norm = df.copy()
    
    # Store min and max values for later use in denormalization
    normalization_params = {}
    
    # Normalize only numeric columns that aren't identifiers or targets
    for col in df_norm.select_dtypes(include=[np.number]).columns:
        if col not in ['engine_id', 'cycle', 'RUL']:
            min_val = df_norm[col].min()
            max_val = df_norm[col].max()
            
            # Store parameters
            normalization_params[col] = {'min': min_val, 'max': max_val}
            
            # Min-max normalization
            if max_val > min_val:  # Avoid division by zero
                df_norm[col] = (df_norm[col] - min_val) / (max_val - min_val)
    
    logger.info("Data normalization completed")
    return df_norm, normalization_params

def split_data(df: pd.DataFrame, test_size: float = 0.2, validation_size: float = 0.1) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    Split data into training, validation, and test sets.
    Ensures that all data for a single engine stays in the same set.
    
    Args:
        df: DataFrame to split
        test_size: Proportion of data for testing
        validation_size: Proportion of data for validation
        
    Returns:
        Tuple of (train_df, val_df, test_df)
    """
    logger.info("Splitting data into train, validation, and test sets...")
    
    # Get unique engine IDs
    engine_ids = df['engine_id'].unique()
    np.random.shuffle(engine_ids)
    
    # Calculate split indices
    n_engines = len(engine_ids)
    n_test = int(n_engines * test_size)
    n_val = int(n_engines * validation_size)
    
    # Split engine IDs
    test_engine_ids = engine_ids[:n_test]
    val_engine_ids = engine_ids[n_test:n_test+n_val]
    train_engine_ids = engine_ids[n_test+n_val:]
    
    # Split dataframe based on engine IDs
    test_df = df[df['engine_id'].isin(test_engine_ids)]
    val_df = df[df['engine_id'].isin(val_engine_ids)]
    train_df = df[df['engine_id'].isin(train_engine_ids)]
    
    logger.info(f"Data split completed. Train: {len(train_df)} rows, Validation: {len(val_df)} rows, Test: {len(test_df)} rows")
    return train_df, val_df, test_df

def prepare_sequence_data(df: pd.DataFrame, sequence_length: int, features: List[str], target: str = 'RUL') -> Tuple[np.ndarray, np.ndarray]:
    """
    Prepare sequence data for time series modeling.
    
    Args:
        df: DataFrame with time series data
        sequence_length: Length of sequences to create
        features: List of feature column names
        target: Target column name
        
    Returns:
        Tuple of (X, y) where X is a 3D array of sequences and y is the target values
    """
    logger.info(f"Preparing sequence data with length {sequence_length}...")
    
    # DEV NOTE: This function is driving me INSANE. Spent 3 days debugging why
    # the sequences weren't aligning properly. Turns out I was forgetting to sort
    # by cycle within each engine_id. Academia never prepared me for this...
    
    X = []
    y = []
    
    # Process each engine separately
    for engine_id in df['engine_id'].unique():
        # Get data for this engine and sort by cycle
        engine_data = df[df['engine_id'] == engine_id].sort_values('cycle')
        
        # Extract features and target
        feature_data = engine_data[features].values
        target_data = engine_data[target].values
        
        # Create sequences
        for i in range(len(engine_data) - sequence_length + 1):
            X.append(feature_data[i:i+sequence_length])
            # Use the target value at the end of the sequence
            y.append(target_data[i+sequence_length-1])
    
    logger.info(f"Created {len(X)} sequences")
    return np.array(X), np.array(y)

def process_data_pipeline(filepath: str, sequence_length: int = 30) -> Dict:
    """
    Complete data processing pipeline.
    
    Args:
        filepath: Path to the raw data file
        sequence_length: Length of sequences for time series models
        
    Returns:
        Dictionary containing processed data and parameters
    """
    logger.info("Starting data processing pipeline...")
    
    # Load data
    df = load_dataset(filepath)
    
    # Clean data
    df_clean = clean_dataset(df)
    
    # Calculate RUL
    df_with_rul = calculate_RUL(df_clean)
    
    # Normalize data
    df_norm, norm_params = normalize_data(df_with_rul)
    
    # Split data
    train_df, val_df, test_df = split_data(df_norm)
    
    # Define features (excluding identifiers and target)
    features = [col for col in df_norm.columns if col not in ['engine_id', 'RUL']]
    
    # Prepare sequence data for each split
    X_train, y_train = prepare_sequence_data(train_df, sequence_length, features)
    X_val, y_val = prepare_sequence_data(val_df, sequence_length, features)
    X_test, y_test = prepare_sequence_data(test_df, sequence_length, features)
    
    logger.info("Data processing pipeline completed")
    
    # Return all processed data and parameters
    return {
        'X_train': X_train,
        'y_train': y_train,
        'X_val': X_val,
        'y_val': y_val,
        'X_test': X_test,
        'y_test': y_test,
        'features': features,
        'normalization_params': norm_params,
        'sequence_length': sequence_length
    }

if __name__ == "__main__":
    # Example usage
    data_dir = "../data/raw"
    filepath = os.path.join(data_dir, "train_FD001.txt")
    
    if os.path.exists(filepath):
        processed_data = process_data_pipeline(filepath)
        logger.info(f"Processed data shapes: X_train: {processed_data['X_train'].shape}, y_train: {processed_data['y_train'].shape}")
    else:
        logger.error(f"File not found: {filepath}")
        logger.info("Please download the NASA Turbofan Engine Degradation Simulation Dataset from Kaggle")